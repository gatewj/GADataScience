{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Algebra and the Regression Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "* Understand how Matrix Algebra is used to find coefficients in an ordinary least squares regression\n",
    "* Review two techniques to improving the linear model\n",
    "* Understanding the use case and results of a Logistic Regression\n",
    "* New Cross Validation Technique - K-Fold\n",
    "* First Metric for Classification: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "practice_set = pd.DataFrame({\n",
    "    'x': [3.385, 0.48, 1.35, 465, 36.33],\n",
    "    'y': [44.5, 15.5, 8.1, 423, 119.5],\n",
    "})\n",
    "\n",
    "A = np.array([[1, 1, 1, 1, 1], practice_set['x']])\n",
    "# print np.linalg.inv(A.dot(A.T)).dot(A.dot(practice_set['y']))\n",
    "\n",
    "model = lm.LinearRegression().fit(practice_set[['x']], practice_set['y'])\n",
    "#print model.intercept_\n",
    "#print model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.200896079333063"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83821876])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.13699530933\n",
      "            Iterations: 30\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 30\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:                  100\n",
      "Model:                          Logit   Df Residuals:                       97\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Sun, 24 Apr 2016   Pseudo R-squ.:                  0.8024\n",
      "Time:                        18:16:06   Log-Likelihood:                -13.700\n",
      "converged:                       True   LL-Null:                       -69.315\n",
      "                                        LLR p-value:                 7.025e-25\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -14.3793      5.097     -2.821      0.005       -24.370    -4.389\n",
      "sep_wid       -3.9071      1.746     -2.237      0.025        -7.330    -0.485\n",
      "pet_wid       15.7003      3.662      4.288      0.000         8.523    22.877\n",
      "==============================================================================\n",
      "Intercept    5.690526e-07\n",
      "sep_wid      2.009806e-02\n",
      "pet_wid      6.584847e+06\n",
      "dtype: float64\n",
      "[[ -3.90710181  15.70023074]]\n",
      "[[  2.00986665e-02   6.58451173e+06]]\n",
      "[[ -3.90532894  15.69410116]]\n",
      "[[  2.01343304e-02   6.54427488e+06]]\n"
     ]
    }
   ],
   "source": [
    "### Statsmodels, because we love statsmodels,\n",
    "### and because logistic regressions are still common for data exploration:from sklearn import datasets.\n",
    "### Since logistic is used to solve binomial problems, our targets should be 0 and 1.\n",
    "### by default, fit_regularized uses L1 Regularization\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "iris = datasets.load_iris()\n",
    "irisdf = pd.DataFrame(iris.data, columns=['sep_len', 'sep_wid', 'pet_len', 'pet_wid'])\n",
    "irisdf['target'] = iris.target\n",
    "irisdf = irisdf.query('target in (1, 2)')\n",
    "irisdf['target'] = irisdf['target'] - 1\n",
    "\n",
    "lmf = smf.logit('target ~ sep_wid + pet_wid', irisdf)\n",
    "# setting alpha to 0 effectively removes the hyperparameter\n",
    "results = lmf.fit_regularized(alpha=0)\n",
    "print results.summary()\n",
    "print np.exp(results.params)\n",
    "\n",
    "# sklearn implementation\n",
    "# note that by default, LogisticRegression() uses L2 Regularization.\n",
    "# C, in this case, is the alpha parameter\n",
    "# we can't remove it (it's a fickle sklearn library issue), but we can make is huge so it as less of an effect.\n",
    "clf = lm.LogisticRegression(C=1e100).fit(irisdf[['sep_wid', 'pet_wid']], irisdf['target'])\n",
    "print clf.coef_\n",
    "print np.exp(clf.coef_)\n",
    "# same model, different regularization method\n",
    "clf2 = lm.LogisticRegression(penalty='l1', C=1e100).fit(irisdf[['sep_wid', 'pet_wid']], irisdf['target'])\n",
    "print clf2.coef_\n",
    "print np.exp(clf2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
